// Lexer for the Shotgun compiler
//
// The lexer reads source code character by character and produces tokens.
// It handles whitespace, comments, string interpolation, and multi-character operators.

// Lexer state
Lexer :: struct {
    source str
    pos int
    line int
    col int
    file str
    keywords Keywords
    nesting int  // Track nesting depth for significant newlines
}

// Create a new lexer
fn new_lexer(str source, str file) Lexer {
    return Lexer {
        source: source,
        pos: 0,
        line: 1,
        col: 1,
        file: file,
        keywords: make_keywords(),
        nesting: 0
    }
}

// Check if we're at end of file
Lexer :: is_eof(self) bool {
    return self.pos >= self.source.len()
}

// Peek at current character without advancing (returns \0 at EOF)
Lexer :: peek(self) char {
    if self.is_eof() {
        return '\0'
    }
    return self.source.at(self.pos)
}

// Peek at next character (one ahead)
Lexer :: peek_next(self) char {
    if self.pos + 1 >= self.source.len() {
        return '\0'
    }
    return self.source.at(self.pos + 1)
}

// Advance position by one character
Lexer :: advance(self) char {
    ch := self.peek()
    self.pos = self.pos + 1
    if ch == '\n' {
        self.line = self.line + 1
        self.col = 1
    } else {
        self.col = self.col + 1
    }
    return ch
}

// Get current location
Lexer :: location(self) Location {
    return Location { line: self.line, col: self.col, file: self.file }
}

// Create a token at the current location
Lexer :: make_token(self, Token tok) LocatedToken {
    return LocatedToken { token: tok, location: self.location() }
}

// Check if character is whitespace
fn is_whitespace(char ch) bool {
    return ch == ' ' || ch == '\t'
}

// Check if character is newline
fn is_newline(char ch) bool {
    return ch == '\n' || ch == '\r'
}

// Check if character is a digit
fn is_digit(char ch) bool {
    return ch >= '0' && ch <= '9'
}

// Check if character is a lowercase letter
fn is_lower(char ch) bool {
    return ch >= 'a' && ch <= 'z'
}

// Check if character is an uppercase letter
fn is_upper(char ch) bool {
    return ch >= 'A' && ch <= 'Z'
}

// Check if character is a letter
fn is_letter(char ch) bool {
    return is_lower(ch) || is_upper(ch)
}

// Check if character can start an identifier (lowercase or underscore)
fn is_ident_start(char ch) bool {
    return is_lower(ch) || ch == '_'
}

// Check if character can start a type identifier (uppercase)
fn is_type_start(char ch) bool {
    return is_upper(ch)
}

// Check if character can be in an identifier
fn is_ident_char(char ch) bool {
    return is_letter(ch) || is_digit(ch) || ch == '_'
}

// Skip whitespace (but not newlines)
Lexer :: skip_whitespace(self) {
    while !self.is_eof() && is_whitespace(self.peek()) {
        self.advance()
    }
}

// Skip a line comment (// to end of line)
Lexer :: skip_line_comment(self) {
    while !self.is_eof() && !is_newline(self.peek()) {
        self.advance()
    }
}

// Read an identifier or keyword
Lexer :: read_ident(self) str {
    start := self.pos
    while !self.is_eof() && is_ident_char(self.peek()) {
        self.advance()
    }
    return self.source.slice(start, self.pos)
}

// Read a type identifier (starts with uppercase)
Lexer :: read_type_ident(self) str {
    start := self.pos
    while !self.is_eof() && is_ident_char(self.peek()) {
        self.advance()
    }
    return self.source.slice(start, self.pos)
}

// Read an integer (returns the string, caller converts)
Lexer :: read_number(self) Token {
    start := self.pos
    while !self.is_eof() && is_digit(self.peek()) {
        self.advance()
    }

    // Check for float
    if self.peek() == '.' && is_digit(self.peek_next()) {
        self.advance()  // consume the dot
        while !self.is_eof() && is_digit(self.peek()) {
            self.advance()
        }
        num_str := self.source.slice(start, self.pos)
        return Token.TkFloat { value: parse_float(num_str) }
    }

    num_str := self.source.slice(start, self.pos)
    return Token.TkInt { value: parse_int(num_str) }
}

// Read a string literal (handles escape sequences and interpolation)
Lexer :: read_string(self) Token {
    self.advance()  // consume opening quote
    result := ""

    while !self.is_eof() && self.peek() != '"' {
        ch := self.peek()

        if ch == '\\' {
            self.advance()
            next := self.peek()
            if next == 'n' {
                result = result + "\n"
                self.advance()
            } else if next == 'r' {
                result = result + "\r"
                self.advance()
            } else if next == 't' {
                result = result + "\t"
                self.advance()
            } else if next == '\\' {
                result = result + "\\"
                self.advance()
            } else if next == '"' {
                result = result + "\""
                self.advance()
            } else if next == '{' {
                result = result + "\\" + "\{"
                self.advance()
            } else {
                result = result + "\\"
            }
        } else if ch == '{' {
            // Read until matching }
            result = result + "\{"
            self.advance()
            depth := 1
            while !self.is_eof() && depth > 0 {
                c := self.peek()
                if c == '{' {
                    depth = depth + 1
                } else if c == '}' {
                    depth = depth - 1
                }
                if depth > 0 {
                    result = result + c.to_string()
                    self.advance()
                }
            }
            result = result + "\}"
            if !self.is_eof() {
                self.advance()  // consume closing }
            }
        } else {
            result = result + ch.to_string()
            self.advance()
        }
    }

    if !self.is_eof() {
        self.advance()  // consume closing quote
    }

    return Token.TkString { value: result }
}

// Read a character literal 'a', '\n', etc.
Lexer :: read_char(self) Token {
    self.advance()  // consume opening quote

    if self.is_eof() {
        return Token.TkError { msg: "unterminated character literal" }
    }

    ch := self.peek()
    result_char := '\0'

    if ch == '\\' {
        self.advance()
        next := self.peek()
        if next == 'n' {
            result_char = '\n'
        } else if next == 'r' {
            result_char = '\r'
        } else if next == 't' {
            result_char = '\t'
        } else if next == '\\' {
            result_char = '\\'
        } else if next == '\'' {
            result_char = '\''
        } else if next == '0' {
            result_char = '\0'
        } else {
            return Token.TkError { msg: "unknown escape sequence in character literal" }
        }
        self.advance()
    } else if ch == '\'' {
        return Token.TkError { msg: "empty character literal" }
    } else {
        result_char = ch
        self.advance()
    }

    // Expect closing quote
    if self.peek() != '\'' {
        return Token.TkError { msg: "unterminated character literal" }
    }
    self.advance()  // consume closing quote

    return Token.TkChar { value: result_char }
}

// Get the next token
Lexer :: next_token(self) LocatedToken {
    // Skip whitespace
    self.skip_whitespace()

    // Handle newlines
    if is_newline(self.peek()) {
        loc := self.location()
        self.advance()
        // Skip \r\n as single newline
        if self.peek() == '\n' {
            self.advance()
        }
        // Only emit newline token if not inside brackets/parens/braces
        if self.nesting == 0 {
            return LocatedToken { token: Token.TkNewline{}, location: loc }
        }
        return self.next_token()  // Skip newline inside nesting
    }

    // Check for EOF
    if self.is_eof() {
        return self.make_token(Token.TkEof{})
    }

    // Check for line comment
    if self.peek() == '/' && self.peek_next() == '/' {
        self.skip_line_comment()
        return self.next_token()
    }

    loc := self.location()
    ch := self.peek()

    // Identifiers and keywords
    if is_ident_start(ch) {
        ident := self.read_ident()
        if ident == "_" {
            return LocatedToken { token: Token.TkUnderscore{}, location: loc }
        }
        if self.keywords.is_keyword(ident) {
            return LocatedToken { token: Token.TkKeyword { name: ident }, location: loc }
        }
        return LocatedToken { token: Token.TkIdent { name: ident }, location: loc }
    }

    // Type identifiers
    if is_type_start(ch) {
        ident := self.read_type_ident()
        return LocatedToken { token: Token.TkTypeIdent { name: ident }, location: loc }
    }

    // Numbers
    if is_digit(ch) {
        tok := self.read_number()
        return LocatedToken { token: tok, location: loc }
    }

    // Strings
    if ch == '"' {
        tok := self.read_string()
        return LocatedToken { token: tok, location: loc }
    }

    // Character literals
    if ch == '\'' {
        tok := self.read_char()
        return LocatedToken { token: tok, location: loc }
    }

    // Multi-character operators and punctuation
    self.advance()

    // Two-character operators
    next := self.peek()

    if ch == ':' {
        if next == ':' {
            self.advance()
            return LocatedToken { token: Token.TkColonColon{}, location: loc }
        }
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkColonEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkColon{}, location: loc }
    }

    if ch == '-' {
        if next == '>' {
            self.advance()
            return LocatedToken { token: Token.TkArrow{}, location: loc }
        }
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkMinusEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkMinus{}, location: loc }
    }

    if ch == '=' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkEqEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkEq{}, location: loc }
    }

    if ch == '!' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkNeq{}, location: loc }
        }
        return LocatedToken { token: Token.TkBang{}, location: loc }
    }

    if ch == '<' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkLte{}, location: loc }
        }
        return LocatedToken { token: Token.TkLt{}, location: loc }
    }

    if ch == '>' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkGte{}, location: loc }
        }
        return LocatedToken { token: Token.TkGt{}, location: loc }
    }

    if ch == '+' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkPlusEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkPlus{}, location: loc }
    }

    if ch == '*' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkStarEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkStar{}, location: loc }
    }

    if ch == '/' {
        if next == '=' {
            self.advance()
            return LocatedToken { token: Token.TkSlashEq{}, location: loc }
        }
        return LocatedToken { token: Token.TkSlash{}, location: loc }
    }

    if ch == '&' {
        if next == '&' {
            self.advance()
            return LocatedToken { token: Token.TkAndAnd{}, location: loc }
        }
        return LocatedToken { token: Token.TkError { msg: "unexpected '&'" }, location: loc }
    }

    if ch == '|' {
        if next == '|' {
            self.advance()
            return LocatedToken { token: Token.TkOrOr{}, location: loc }
        }
        return LocatedToken { token: Token.TkError { msg: "unexpected '|'" }, location: loc }
    }

    // Single-character tokens with nesting tracking
    if ch == '(' {
        self.nesting = self.nesting + 1
        return LocatedToken { token: Token.TkLParen{}, location: loc }
    }
    if ch == ')' {
        self.nesting = self.nesting - 1
        return LocatedToken { token: Token.TkRParen{}, location: loc }
    }
    if ch == '{' {
        self.nesting = self.nesting + 1
        return LocatedToken { token: Token.TkLBrace{}, location: loc }
    }
    if ch == '}' {
        self.nesting = self.nesting - 1
        return LocatedToken { token: Token.TkRBrace{}, location: loc }
    }
    if ch == '[' {
        self.nesting = self.nesting + 1
        return LocatedToken { token: Token.TkLBracket{}, location: loc }
    }
    if ch == ']' {
        self.nesting = self.nesting - 1
        return LocatedToken { token: Token.TkRBracket{}, location: loc }
    }

    // Other single-character tokens
    if ch == ',' {
        return LocatedToken { token: Token.TkComma{}, location: loc }
    }
    if ch == '.' {
        return LocatedToken { token: Token.TkDot{}, location: loc }
    }
    if ch == '%' {
        return LocatedToken { token: Token.TkPercent{}, location: loc }
    }
    if ch == '?' {
        return LocatedToken { token: Token.TkQuestion{}, location: loc }
    }

    // Unknown character
    return LocatedToken {
        token: Token.TkError { msg: "unexpected character" },
        location: loc
    }
}

// Parse integer from string
fn parse_int(str s) int {
    result := 0
    i := 0
    while i < s.len() {
        ch := s.at(i)
        if ch >= '0' && ch <= '9' {
            result = result * 10 + (ord(ch) - ord('0'))
        }
        i = i + 1
    }
    return result
}

// Parse float from string (simple implementation)
fn parse_float(str s) f64 {
    // Find the dot
    dot_pos := s.find(".")
    if dot_pos < 0 {
        return 0.0  // No dot, shouldn't happen
    }

    int_part := s.slice(0, dot_pos)
    frac_part := s.slice(dot_pos + 1, s.len())

    int_val := parse_int(int_part)
    frac_val := parse_int(frac_part)

    // Calculate divisor based on fraction length
    divisor := 1.0
    i := 0
    while i < frac_part.len() {
        divisor = divisor * 10.0
        i = i + 1
    }

    // Convert to float for the result
    f64 int_f = int_val * 1.0
    f64 frac_f = frac_val * 1.0
    return int_f + frac_f / divisor
}
