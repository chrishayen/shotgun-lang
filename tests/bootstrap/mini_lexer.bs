// Mini-lexer test - tokenize a simple string
// This demonstrates the core pattern needed for a self-hosting compiler lexer

// Token type
Token :: variant {
    TkIdent { value str }
    TkInt { value int }
    TkKeyword { name str }
    TkSymbol { ch str }
    TkEof
}

// Lexer state
Lexer :: struct {
    source str
    pos int
}

// Check if character is whitespace (space=32, tab=9, newline=10, cr=13)
fn is_whitespace(int ch) bool {
    return ch in [32, 9, 10, 13]
}

// Check if character is a digit (0-9: 48-57)
fn is_digit(int ch) bool {
    return ch >= 48 && ch <= 57
}

// Check if character is a letter (a-z: 97-122, A-Z: 65-90)
fn is_letter(int ch) bool {
    return (ch >= 97 && ch <= 122) || (ch >= 65 && ch <= 90)
}

// Check if character can start an identifier
fn is_ident_start(int ch) bool {
    return is_letter(ch) || ch == 95
}

// Check if character can be in an identifier
fn is_ident_char(int ch) bool {
    return is_letter(ch) || is_digit(ch) || ch == 95
}

// Peek at current character without advancing
Lexer :: peek(self) int {
    if self.pos >= self.source.len() {
        return 0
    }
    return self.source.at(self.pos)
}

// Advance position
Lexer :: advance(self) {
    self.pos = self.pos + 1
}

// Skip whitespace
Lexer :: skip_whitespace(self) {
    while self.pos < self.source.len() && is_whitespace(self.peek()) {
        self.advance()
    }
}

// Read an identifier or keyword
Lexer :: read_ident(self) str {
    int start = self.pos
    while self.pos < self.source.len() && is_ident_char(self.peek()) {
        self.advance()
    }
    return self.source.slice(start, self.pos)
}

// Read a number
Lexer :: read_number(self) int {
    int value = 0
    while self.pos < self.source.len() && is_digit(self.peek()) {
        int digit = self.peek() - 48
        value = value * 10 + digit
        self.advance()
    }
    return value
}

// Get next token
Lexer :: next_token(self) Token {
    self.skip_whitespace()

    if self.pos >= self.source.len() {
        return Token.TkEof{}
    }

    int ch = self.peek()

    // Identifiers and keywords
    if is_ident_start(ch) {
        str ident = self.read_ident()
        // Check for keywords
        if ident in ["fn", "if", "else", "return", "while", "for"] {
            return Token.TkKeyword{ name: ident }
        }
        return Token.TkIdent{ value: ident }
    }

    // Numbers
    if is_digit(ch) {
        int num = self.read_number()
        return Token.TkInt{ value: num }
    }

    // Single character symbols
    self.advance()
    str sym = self.source.slice(self.pos - 1, self.pos)
    return Token.TkSymbol{ ch: sym }
}

// Print a token
fn print_token(Token tok) {
    match tok {
        Token.TkIdent{ value: v } -> print("IDENT({v})"),
        Token.TkInt{ value: n } -> print("INT({n})"),
        Token.TkKeyword{ name: k } -> print("KW({k})"),
        Token.TkSymbol{ ch: s } -> print("SYM({s})"),
        Token.TkEof -> print("EOF")
    }
}

fn main {
    str source = "fn main x = 42 + y"
    Lexer lex = Lexer{ source: source, pos: 0 }

    // Tokenize and print all tokens
    bool done = false
    while !done {
        Token tok = lex.next_token()
        print_token(tok)
        match tok {
            Token.TkEof -> done = true,
            _ -> done = false
        }
    }

    print("Lexer test complete")
}
